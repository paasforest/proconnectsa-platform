from celery import shared_task
from django.conf import settings
from django.utils import timezone
from datetime import timedelta
import logging
from backend.leads.client_behavior_ml import ClientBehaviorML

logger = logging.getLogger(__name__)


@shared_task
def retrain_models_if_thresholds_met():
    """Periodically retrain ML models if enough data exists."""
    from django.core.management import call_command
    from backend.leads.models import Lead, LeadAssignment
    from backend.leads.ml_services import LeadQualityMLService, LeadConversionMLService, GeographicalMLService

    min_leads = getattr(settings, 'ML_MIN_QUALITY_TRAINING_LEADS', 50)
    min_assignments = getattr(settings, 'ML_MIN_CONVERSION_TRAINING_ASSIGNMENTS', 30)
    min_geographical_assignments = getattr(settings, 'ML_MIN_GEOGRAPHICAL_TRAINING_ASSIGNMENTS', 30)

    # Check data from last 90 days
    lead_count = Lead.objects.filter(
        created_at__gte=timezone.now() - timedelta(days=90)
    ).count()
    assignment_count = LeadAssignment.objects.filter(
        assigned_at__gte=timezone.now() - timedelta(days=90)
    ).count()

    logger.info(f"ML Training Check: {lead_count} leads, {assignment_count} assignments")

    ran_any = False
    results = {}

    # Train quality model if enough leads
    if lead_count >= min_leads:
        try:
            logger.info("Training quality model...")
            quality_ml = LeadQualityMLService()
            success = quality_ml.train_quality_model()
            if success:
                ran_any = True
                results['quality_model'] = 'trained'
                logger.info("Quality model trained successfully")
            else:
                results['quality_model'] = 'failed'
                logger.warning("Quality model training failed")
        except Exception as e:
            logger.error(f"Error training quality model: {str(e)}")
            results['quality_model'] = f'error: {str(e)}'

    # Train conversion model if enough assignments
    if assignment_count >= min_assignments:
        try:
            logger.info("Training conversion model...")
            conversion_ml = LeadConversionMLService()
            success = conversion_ml.train_conversion_model()
            if success:
                ran_any = True
                results['conversion_model'] = 'trained'
                logger.info("Conversion model trained successfully")
            else:
                results['conversion_model'] = 'failed'
                logger.warning("Conversion model training failed")
        except Exception as e:
            logger.error(f"Error training conversion model: {str(e)}")
            results['conversion_model'] = f'error: {str(e)}'

    # Train geographical model if enough assignments
    if assignment_count >= min_geographical_assignments:
        try:
            logger.info("Training geographical model...")
            geographical_ml = GeographicalMLService()
            success = geographical_ml.train_geographical_model()
            if success:
                ran_any = True
                results['geographical_model'] = 'trained'
                logger.info("Geographical model trained successfully")
            else:
                results['geographical_model'] = 'failed'
                logger.warning("Geographical model training failed")
        except Exception as e:
            logger.error(f"Error training geographical model: {str(e)}")
            results['geographical_model'] = f'error: {str(e)}'

    return {
        'ran_any': ran_any,
        'lead_count': lead_count,
        'assignment_count': assignment_count,
        'results': results,
        'timestamp': timezone.now().isoformat()
    }


@shared_task
def train_geographical_ml_model():
    """Train geographical ML model for proximity-based lead matching"""
    from backend.leads.ml_services import GeographicalMLService
    from backend.leads.models import LeadAssignment
    
    logger.info("Starting geographical ML model training...")
    
    try:
        # Check if we have enough data
        assignment_count = LeadAssignment.objects.filter(
            assigned_at__gte=timezone.now() - timedelta(days=90),
            status__in=['accepted', 'completed']
        ).count()
        
        min_assignments = getattr(settings, 'ML_MIN_GEOGRAPHICAL_TRAINING_ASSIGNMENTS', 30)
        
        if assignment_count < min_assignments:
            logger.warning(f"Not enough assignments for geographical training: {assignment_count} < {min_assignments}")
            return {
                'success': False,
                'message': f'Not enough training data: {assignment_count} assignments',
                'timestamp': timezone.now().isoformat()
            }
        
        # Train the model
        geographical_ml = GeographicalMLService()
        success = geographical_ml.train_geographical_model()
        
        if success:
            logger.info("Geographical ML model trained successfully")
            return {
                'success': True,
                'message': 'Geographical ML model trained successfully',
                'assignment_count': assignment_count,
                'timestamp': timezone.now().isoformat()
            }
        else:
            logger.error("Geographical ML model training failed")
            return {
                'success': False,
                'message': 'Geographical ML model training failed',
                'assignment_count': assignment_count,
                'timestamp': timezone.now().isoformat()
            }
            
    except Exception as e:
        logger.error(f"Error training geographical ML model: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def full_ml_model_retrain():
    """Full retrain of all ML models weekly."""
    from django.core.management import call_command
    from backend.leads.models import Lead, LeadAssignment
    from backend.leads.ml_services import LeadQualityMLService, LeadConversionMLService, GeographicalMLService

    logger.info("Starting full ML model retrain...")

    # Get data counts
    lead_count = Lead.objects.filter(
        created_at__gte=timezone.now() - timedelta(days=180)  # 6 months
    ).count()
    assignment_count = LeadAssignment.objects.filter(
        assigned_at__gte=timezone.now() - timedelta(days=180)
    ).count()

    results = {}

    # Always retrain quality model if we have any data
    if lead_count > 0:
        try:
            logger.info("Full retraining quality model...")
            quality_ml = LeadQualityMLService()
            success = quality_ml.train_quality_model()
            results['quality_model'] = 'trained' if success else 'failed'
        except Exception as e:
            logger.error(f"Error in full quality model retrain: {str(e)}")
            results['quality_model'] = f'error: {str(e)}'

    # Always retrain conversion model if we have any data
    if assignment_count > 0:
        try:
            logger.info("Full retraining conversion model...")
            conversion_ml = LeadConversionMLService()
            success = conversion_ml.train_conversion_model()
            results['conversion_model'] = 'trained' if success else 'failed'
        except Exception as e:
            logger.error(f"Error in full conversion model retrain: {str(e)}")
            results['conversion_model'] = f'error: {str(e)}'

    logger.info("Full ML model retrain completed")
    return {
        'lead_count': lead_count,
        'assignment_count': assignment_count,
        'results': results,
        'timestamp': timezone.now().isoformat()
    }


@shared_task
def reset_monthly_lead_usage():
    """Reset monthly lead usage for all providers."""
    from backend.users.models import ProviderProfile

    logger.info("Resetting monthly lead usage for all providers...")

    try:
        # Reset all provider monthly usage
        updated_count = ProviderProfile.objects.update(leads_used_this_month=0)
        
        logger.info(f"Reset monthly usage for {updated_count} providers")
        
        return {
            'updated_providers': updated_count,
            'timestamp': timezone.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error resetting monthly lead usage: {str(e)}")
        return {
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def learn_from_deposit_patterns():
    """Learn from deposit patterns to improve ML models"""
    from backend.payments.models import ManualDeposit, CreditTransaction
    from backend.users.models import ProviderProfile
    from backend.leads.ml_services import LeadAccessControlMLService
    from django.utils import timezone
    from datetime import timedelta

    logger.info("Learning from deposit patterns...")

    try:
        # Get recent deposits for learning
        recent_deposits = ManualDeposit.objects.filter(
            created_at__gte=timezone.now() - timedelta(days=30),
            status='verified'
        )

        if recent_deposits.count() < 10:
            logger.info(f"Not enough recent deposits ({recent_deposits.count()}) for pattern learning")
            return {
                'learned': False,
                'reason': 'insufficient_data',
                'deposits_analyzed': recent_deposits.count(),
                'timestamp': timezone.now().isoformat()
            }

        # Analyze deposit patterns
        patterns = {
            'total_deposits': recent_deposits.count(),
            'total_amount': sum(float(d.amount) for d in recent_deposits),
            'average_amount': 0,
            'subscription_vs_payg': {'subscription': 0, 'payg': 0},
            'credit_efficiency': 0,
            'fraud_indicators': []
        }

        # Calculate patterns
        if patterns['total_deposits'] > 0:
            patterns['average_amount'] = patterns['total_amount'] / patterns['total_deposits']

        # Analyze by provider type
        for deposit in recent_deposits:
            provider = deposit.provider.provider_profile
            if provider.is_subscription_active:
                patterns['subscription_vs_payg']['subscription'] += 1
            else:
                patterns['subscription_vs_payg']['payg'] += 1

        # Calculate credit efficiency
        total_credits = sum(d.credits_to_activate for d in recent_deposits)
        if patterns['total_amount'] > 0:
            patterns['credit_efficiency'] = total_credits / patterns['total_amount']

        # Detect potential fraud patterns
        for deposit in recent_deposits:
            # Check for unusual amounts
            if float(deposit.amount) > patterns['average_amount'] * 3:
                patterns['fraud_indicators'].append({
                    'type': 'unusual_amount',
                    'deposit_id': str(deposit.id),
                    'amount': float(deposit.amount),
                    'provider': deposit.provider.email
                })

        logger.info(f"Deposit pattern learning completed: {patterns}")
        
        return {
            'learned': True,
            'patterns': patterns,
            'deposits_analyzed': recent_deposits.count(),
            'timestamp': timezone.now().isoformat()
        }

    except Exception as e:
        logger.error(f"Error learning from deposit patterns: {str(e)}")
        return {
            'learned': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def train_support_ml_models():
    """Train support ticket ML models"""
    from backend.support.ml_services import SupportTicketMLService
    from django.utils import timezone

    logger.info("Starting support ML model training...")

    try:
        ml_service = SupportTicketMLService()
        result = ml_service.retrain_models()
        
        if result['success']:
            logger.info("Support ML models trained successfully")
            return {
                'success': True,
                'message': 'Support ML models trained successfully',
                'timestamp': timezone.now().isoformat()
            }
        else:
            logger.error(f"Support ML training failed: {result['error']}")
            return {
                'success': False,
                'error': result['error'],
                'timestamp': timezone.now().isoformat()
            }

    except Exception as e:
        logger.error(f"Error training support ML models: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def analyze_support_ticket_sentiment():
    """Analyze sentiment of recent support tickets"""
    from backend.support.models import SupportTicket
    from backend.support.ml_services import SupportTicketMLService
    from django.utils import timezone
    from datetime import timedelta

    logger.info("Analyzing support ticket sentiment...")

    try:
        # Get recent tickets without sentiment analysis
        recent_tickets = SupportTicket.objects.filter(
            created_at__gte=timezone.now() - timedelta(days=7),
            satisfaction_rating__isnull=True
        )

        if recent_tickets.count() < 5:
            logger.info(f"Not enough recent tickets ({recent_tickets.count()}) for sentiment analysis")
            return {
                'analyzed': False,
                'reason': 'insufficient_data',
                'tickets_analyzed': recent_tickets.count(),
                'timestamp': timezone.now().isoformat()
            }

        ml_service = SupportTicketMLService()
        analyzed_count = 0

        for ticket in recent_tickets:
            try:
                # Get sentiment prediction
                sentiment_result = ml_service.predict_sentiment(
                    ticket.title,
                    ticket.description,
                    ticket.user_type
                )

                # Store sentiment analysis (you could add a sentiment field to the model)
                logger.info(f"Ticket {ticket.ticket_number} sentiment: {sentiment_result['sentiment']}")
                analyzed_count += 1

            except Exception as e:
                logger.error(f"Error analyzing sentiment for ticket {ticket.id}: {str(e)}")

        logger.info(f"Support ticket sentiment analysis completed: {analyzed_count} tickets analyzed")
        
        return {
            'analyzed': True,
            'tickets_analyzed': analyzed_count,
            'timestamp': timezone.now().isoformat()
        }

    except Exception as e:
        logger.error(f"Error analyzing support ticket sentiment: {str(e)}")
        return {
            'analyzed': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def optimize_support_ticket_assignments():
    """Optimize support ticket assignments using ML"""
    from backend.support.models import SupportTicket
    from backend.support.ml_services import SupportTicketMLService
    from django.contrib.auth import get_user_model
    from django.utils import timezone

    User = get_user_model()
    logger.info("Optimizing support ticket assignments...")

    try:
        # Get unassigned tickets
        unassigned_tickets = SupportTicket.objects.filter(
            assigned_to__isnull=True,
            status__in=['open', 'in_progress']
        )

        if unassigned_tickets.count() == 0:
            logger.info("No unassigned tickets to optimize")
            return {
                'optimized': False,
                'reason': 'no_unassigned_tickets',
                'tickets_optimized': 0,
                'timestamp': timezone.now().isoformat()
            }

        ml_service = SupportTicketMLService()
        optimized_count = 0

        for ticket in unassigned_tickets:
            try:
                # Get auto-assignment suggestion
                assignment_result = ml_service.suggest_auto_assignment(
                    ticket.title,
                    ticket.description,
                    ticket.user_type
                )

                if assignment_result['suggested_staff_id'] and assignment_result['confidence'] > 0.7:
                    # Find the suggested staff member
                    try:
                        suggested_staff = User.objects.get(
                            id=assignment_result['suggested_staff_id'],
                            is_staff=True
                        )
                        
                        # Assign the ticket
                        ticket.assign_to(suggested_staff)
                        logger.info(f"Ticket {ticket.ticket_number} auto-assigned to {suggested_staff.email}")
                        optimized_count += 1

                    except User.DoesNotExist:
                        logger.warning(f"Suggested staff member {assignment_result['suggested_staff_id']} not found")

            except Exception as e:
                logger.error(f"Error optimizing assignment for ticket {ticket.id}: {str(e)}")

        logger.info(f"Support ticket assignment optimization completed: {optimized_count} tickets optimized")
        
        return {
            'optimized': True,
            'tickets_optimized': optimized_count,
            'timestamp': timezone.now().isoformat()
        }

    except Exception as e:
        logger.error(f"Error optimizing support ticket assignments: {str(e)}")
        return {
            'optimized': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def generate_support_ml_insights():
    """Generate ML insights for support tickets"""
    from backend.support.models import SupportTicket, SupportMetrics
    from backend.support.ml_services import SupportTicketMLService
    from django.utils import timezone
    from datetime import timedelta
    from django.db.models import Count, Avg

    logger.info("Generating support ML insights...")

    try:
        # Get recent tickets for analysis
        recent_tickets = SupportTicket.objects.filter(
            created_at__gte=timezone.now() - timedelta(days=30)
        )

        if recent_tickets.count() < 10:
            logger.info(f"Not enough recent tickets ({recent_tickets.count()}) for ML insights")
            return {
                'insights_generated': False,
                'reason': 'insufficient_data',
                'tickets_analyzed': recent_tickets.count(),
                'timestamp': timezone.now().isoformat()
            }

        ml_service = SupportTicketMLService()
        insights = {
            'total_tickets': recent_tickets.count(),
            'category_predictions': {},
            'priority_predictions': {},
            'sentiment_analysis': {},
            'response_time_predictions': {},
            'satisfaction_predictions': {}
        }

        # Analyze each ticket
        for ticket in recent_tickets:
            try:
                # Get ML recommendations
                recommendations = ml_service.get_ml_recommendations(
                    ticket.title,
                    ticket.description,
                    ticket.user_type
                )

                if recommendations['success']:
                    recs = recommendations['recommendations']
                    
                    # Track category predictions
                    category = recs['category']['category']
                    insights['category_predictions'][category] = insights['category_predictions'].get(category, 0) + 1
                    
                    # Track priority predictions
                    priority = recs['priority']['priority']
                    insights['priority_predictions'][priority] = insights['priority_predictions'].get(priority, 0) + 1
                    
                    # Track sentiment analysis
                    sentiment = recs['sentiment']['sentiment']
                    insights['sentiment_analysis'][sentiment] = insights['sentiment_analysis'].get(sentiment, 0) + 1

            except Exception as e:
                logger.error(f"Error generating insights for ticket {ticket.id}: {str(e)}")

        # Calculate average response time prediction
        response_times = []
        for ticket in recent_tickets:
            try:
                response_time_result = ml_service.predict_response_time(
                    ticket.title,
                    ticket.description,
                    ticket.user_type
                )
                response_times.append(response_time_result['response_time_hours'])
            except:
                pass

        if response_times:
            insights['avg_predicted_response_time'] = sum(response_times) / len(response_times)

        # Calculate average satisfaction prediction
        satisfaction_ratings = []
        for ticket in recent_tickets:
            try:
                satisfaction_result = ml_service.predict_satisfaction(
                    ticket.title,
                    ticket.description,
                    ticket.user_type
                )
                satisfaction_ratings.append(satisfaction_result['satisfaction_rating'])
            except:
                pass

        if satisfaction_ratings:
            insights['avg_predicted_satisfaction'] = sum(satisfaction_ratings) / len(satisfaction_ratings)

        logger.info(f"Support ML insights generated: {insights}")
        
        return {
            'insights_generated': True,
            'insights': insights,
            'tickets_analyzed': recent_tickets.count(),
            'timestamp': timezone.now().isoformat()
        }

    except Exception as e:
        logger.error(f"Error generating support ML insights: {str(e)}")
        return {
            'insights_generated': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def train_ml_models():
    """Background task to retrain ML models when ready"""
    from backend.leads.ml_monitoring import MLReadinessMonitor
    from backend.leads.ml_services import LeadQualityMLService, LeadConversionMLService, DynamicPricingMLService
    from backend.leads.ml_models import MLModelTrainingLog, MLModelPerformance
    from django.conf import settings
    from django.utils import timezone
    import logging

    logger = logging.getLogger(__name__)
    logger.info("Starting ML model training task...")

    try:
        # Check if ML is enabled
        if not getattr(settings, 'ML_ENABLED', False):
            logger.info("ML is disabled, skipping training")
            return "ML is disabled"

        # Check which services are ready
        status = MLReadinessMonitor.get_ml_readiness_status()
        ready_services = [name for name, data in status.items() if data.get('ready', False)]
        
        if not ready_services:
            logger.info("No ML services are ready for training")
            return "No services ready"

        results = []
        
        # Train Lead Quality Model
        if 'LeadQualityMLService' in ready_services:
            try:
                logger.info("Training Lead Quality ML Service...")
                service = LeadQualityMLService()
                
                # Log training start
                training_log = MLModelTrainingLog.objects.create(
                    model_name='LeadQualityMLService',
                    model_type='lead_quality',
                    status='started',
                    training_data_size=status['LeadQualityMLService'].get('min_leads', 0),
                    features_count=10,  # Would be calculated from actual features
                    hyperparameters={'algorithm': 'random_forest', 'n_estimators': 100}
                )
                
                # Train the model (placeholder - would call actual training)
                # service.train_model()
                
                # Log completion
                training_log.status = 'completed'
                training_log.training_duration_minutes = 15.0  # Placeholder
                
                results.append("LeadQualityMLService trained successfully")
                logger.info("Lead Quality ML Service training completed")
                
            except Exception as e:
                logger.error(f"Error training Lead Quality ML Service: {str(e)}")
                results.append(f"LeadQualityMLService failed: {str(e)}")
        
        # Train Provider Behavior ML Model
        try:
            logger.info("Training Provider Behavior ML Service...")
            from backend.leads.provider_behavior_ml import ProviderBehaviorML
            
            # Log training start
            training_log = MLModelTrainingLog.objects.create(
                model_name='ProviderBehaviorML',
                model_type='provider_behavior',
                status='started',
                training_data_size=0,  # Will be calculated during training
                features_count=15,  # Provider behavior features
                hyperparameters={'algorithm': 'gradient_boosting', 'n_estimators': 100}
            )
            
            # Train the provider behavior models
            service = ProviderBehaviorML()
            result = service.train_all_models(days_back=90)
            
            if 'error' not in result:
                # Log completion
                training_log.status = 'completed'
                training_log.training_duration_minutes = 20.0  # Estimate
                training_log.training_data_size = result.get('samples_used', 0)
                training_log.save()
                
                results.append(f"ProviderBehaviorML trained successfully ({result['samples_used']} samples)")
                logger.info("Provider Behavior ML Service training completed")
            else:
                # Log failure
                training_log.status = 'failed'
                training_log.error_message = result['error']
                training_log.save()
                
                results.append(f"ProviderBehaviorML failed: {result['error']}")
                logger.warning(f"Provider Behavior ML training failed: {result['error']}")
                
        except Exception as e:
            logger.error(f"Error training Provider Behavior ML Service: {str(e)}")
            results.append(f"ProviderBehaviorML failed: {str(e)}")

        # Train Conversion Model
        if 'LeadConversionMLService' in ready_services:
            try:
                logger.info("Training Lead Conversion ML Service...")
                # Similar training logic
                results.append("LeadConversionMLService trained successfully")
            except Exception as e:
                logger.error(f"Failed to train Lead Conversion ML Service: {e}")
                results.append(f"LeadConversionMLService training failed: {e}")

        # Train Dynamic Pricing Model
        if 'DynamicPricingMLService' in ready_services:
            try:
                logger.info("Training Dynamic Pricing ML Service...")
                # Similar training logic
                results.append("DynamicPricingMLService trained successfully")
            except Exception as e:
                logger.error(f"Failed to train Dynamic Pricing ML Service: {e}")
                results.append(f"DynamicPricingMLService training failed: {e}")

        logger.info(f"ML training completed: {', '.join(results)}")
        return "; ".join(results)

    except Exception as e:
        logger.error(f"ML training task failed: {e}")
        return f"Training failed: {e}"


@shared_task
def monitor_ml_model_performance():
    """Monitor ML model performance and log metrics."""
    from backend.leads.models import Lead, LeadAssignment, PredictionLog
    from backend.leads.ml_models import MLModelPerformance, MLPredictionLog
    from django.utils import timezone
    from datetime import timedelta

    logger.info("Monitoring ML model performance...")

    try:
        # Get recent predictions (last 7 days)
        recent_predictions = MLPredictionLog.objects.filter(
            created_at__gte=timezone.now() - timedelta(days=7)
        )

        # Count predictions by type
        quality_predictions = recent_predictions.filter(prediction_type='quality').count()
        conversion_predictions = recent_predictions.filter(prediction_type='conversion').count()

        # Get recent leads and assignments
        recent_leads = Lead.objects.filter(
            created_at__gte=timezone.now() - timedelta(days=7)
        ).count()
        recent_assignments = LeadAssignment.objects.filter(
            assigned_at__gte=timezone.now() - timedelta(days=7)
        ).count()

        metrics = {
            'quality_predictions_7d': quality_predictions,
            'conversion_predictions_7d': conversion_predictions,
            'recent_leads_7d': recent_leads,
            'recent_assignments_7d': recent_assignments,
            'timestamp': timezone.now().isoformat()
        }

        logger.info(f"ML Performance Metrics: {metrics}")
        return metrics

    except Exception as e:
        logger.error(f"Error monitoring ML performance: {str(e)}")
        return {'error': str(e)}


@shared_task
def cleanup_old_prediction_logs():
    """Clean up old prediction logs to prevent database bloat."""
    from backend.leads.models import PredictionLog
    from django.utils import timezone
    from datetime import timedelta

    logger.info("Cleaning up old prediction logs...")

    try:
        # Delete logs older than 90 days
        cutoff_date = timezone.now() - timedelta(days=90)
        deleted_count, _ = PredictionLog.objects.filter(
            created_at__lt=cutoff_date
        ).delete()

        logger.info(f"Deleted {deleted_count} old prediction logs")
        return {
            'deleted_logs': deleted_count,
            'timestamp': timezone.now().isoformat()
        }

    except Exception as e:
        logger.error(f"Error cleaning up prediction logs: {str(e)}")
        return {'error': str(e)}


@shared_task
def train_dynamic_pricing_model():
    """Train the dynamic pricing ML model with new data."""
    from backend.leads.ml_services import DynamicPricingMLService
    from backend.leads.models import Lead, LeadAssignment
    from django.utils import timezone
    from datetime import timedelta

    logger.info("Training dynamic pricing model...")

    try:
        # Check if we have enough recent data (last 30 days)
        recent_assignments = LeadAssignment.objects.filter(
            assigned_at__gte=timezone.now() - timedelta(days=30)
        ).count()

        if recent_assignments < 10:  # Minimum threshold
            logger.info(f"Not enough recent assignments ({recent_assignments}) for pricing model training")
            return {
                'trained': False,
                'reason': 'insufficient_data',
                'recent_assignments': recent_assignments,
                'timestamp': timezone.now().isoformat()
            }

        # Train the pricing model
        pricing_ml = DynamicPricingMLService()
        success = pricing_ml.train_pricing_model()

        if success:
            logger.info("Dynamic pricing model trained successfully")
            return {
                'trained': True,
                'recent_assignments': recent_assignments,
                'timestamp': timezone.now().isoformat()
            }
        else:
            logger.warning("Dynamic pricing model training failed")
            return {
                'trained': False,
                'reason': 'training_failed',
                'recent_assignments': recent_assignments,
                'timestamp': timezone.now().isoformat()
            }

    except Exception as e:
        logger.error(f"Error training dynamic pricing model: {str(e)}")
        return {
            'trained': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def train_lead_access_control_model():
    """Train the lead access control ML model with new patterns."""
    from backend.leads.ml_services import LeadAccessControlMLService
    from backend.users.models import ProviderProfile
    from backend.leads.models import LeadAssignment
    from django.utils import timezone
    from datetime import timedelta

    logger.info("Training lead access control model...")

    try:
        # Check if we have enough recent access patterns (last 30 days)
        recent_accesses = LeadAssignment.objects.filter(
            assigned_at__gte=timezone.now() - timedelta(days=30)
        ).count()

        if recent_accesses < 20:  # Minimum threshold
            logger.info(f"Not enough recent accesses ({recent_accesses}) for access control training")
            return {
                'trained': False,
                'reason': 'insufficient_data',
                'recent_accesses': recent_accesses,
                'timestamp': timezone.now().isoformat()
            }

        # Train the access control model
        access_ml = LeadAccessControlMLService()
        success = access_ml.train_access_model()

        if success:
            logger.info("Lead access control model trained successfully")
            return {
                'trained': True,
                'recent_accesses': recent_accesses,
                'timestamp': timezone.now().isoformat()
            }
        else:
            logger.warning("Lead access control model training failed")
            return {
                'trained': False,
                'reason': 'training_failed',
                'recent_accesses': recent_accesses,
                'timestamp': timezone.now().isoformat()
            }

    except Exception as e:
        logger.error(f"Error training lead access control model: {str(e)}")
        return {
            'trained': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def incremental_ml_learning():
    """Incremental learning from new data every few hours."""
    from backend.leads.ml_services import LeadQualityMLService, LeadConversionMLService, DynamicPricingMLService
    from backend.leads.models import Lead, LeadAssignment
    from django.utils import timezone
    from datetime import timedelta

    logger.info("Starting incremental ML learning...")

    try:
        # Get data from last 6 hours for incremental learning
        recent_leads = Lead.objects.filter(
            created_at__gte=timezone.now() - timedelta(hours=6)
        )
        recent_assignments = LeadAssignment.objects.filter(
            assigned_at__gte=timezone.now() - timedelta(hours=6)
        )

        results = {
            'recent_leads': recent_leads.count(),
            'recent_assignments': recent_assignments.count(),
            'timestamp': timezone.now().isoformat()
        }

        # Only do incremental learning if we have new data
        if recent_leads.count() > 0 or recent_assignments.count() > 0:
            logger.info(f"Incremental learning: {recent_leads.count()} new leads, {recent_assignments.count()} new assignments")

            # Incremental quality model update
            if recent_leads.count() > 0:
                try:
                    quality_ml = LeadQualityMLService()
                    # This would be an incremental update method if implemented
                    results['quality_incremental'] = 'attempted'
                except Exception as e:
                    logger.error(f"Error in incremental quality learning: {str(e)}")
                    results['quality_incremental'] = f'error: {str(e)}'

            # Incremental conversion model update
            if recent_assignments.count() > 0:
                try:
                    conversion_ml = LeadConversionMLService()
                    # This would be an incremental update method if implemented
                    results['conversion_incremental'] = 'attempted'
                except Exception as e:
                    logger.error(f"Error in incremental conversion learning: {str(e)}")
                    results['conversion_incremental'] = f'error: {str(e)}'

            logger.info("Incremental ML learning completed")
        else:
            logger.info("No new data for incremental learning")
            results['status'] = 'no_new_data'

        return results

    except Exception as e:
        logger.error(f"Error in incremental ML learning: {str(e)}")
        return {
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def validate_ml_data_quality():
    """Validate data quality for ML training."""
    from backend.leads.models import Lead, LeadAssignment
    from backend.users.models import ProviderProfile
    from backend.payments.models import CreditTransaction
    from django.utils import timezone
    from datetime import timedelta

    logger.info("Validating ML data quality...")

    try:
        # Check data completeness and quality
        validation_results = {
            'leads': {'total': 0, 'valid': 0, 'issues': []},
            'assignments': {'total': 0, 'valid': 0, 'issues': []},
            'providers': {'total': 0, 'valid': 0, 'issues': []},
            'transactions': {'total': 0, 'valid': 0, 'issues': []},
            'overall_quality': 0.0
        }

        # Validate leads
        leads = Lead.objects.all()
        validation_results['leads']['total'] = leads.count()
        
        valid_leads = 0
        for lead in leads:
            if lead.title and lead.description and lead.service_category:
                valid_leads += 1
            else:
                validation_results['leads']['issues'].append(f"Lead {lead.id} missing required fields")
        
        validation_results['leads']['valid'] = valid_leads

        # Validate assignments
        assignments = LeadAssignment.objects.all()
        validation_results['assignments']['total'] = assignments.count()
        
        valid_assignments = 0
        for assignment in assignments:
            if assignment.lead and assignment.provider and assignment.assigned_at:
                valid_assignments += 1
            else:
                validation_results['assignments']['issues'].append(f"Assignment {assignment.id} missing required fields")
        
        validation_results['assignments']['valid'] = valid_assignments

        # Validate providers
        providers = ProviderProfile.objects.all()
        validation_results['providers']['total'] = providers.count()
        
        valid_providers = 0
        for provider in providers:
            if provider.user and provider.business_name:
                valid_providers += 1
            else:
                validation_results['providers']['issues'].append(f"Provider {provider.id} missing required fields")
        
        validation_results['providers']['valid'] = valid_providers

        # Validate transactions
        transactions = CreditTransaction.objects.all()
        validation_results['transactions']['total'] = transactions.count()
        
        valid_transactions = 0
        for transaction in transactions:
            if transaction.user and transaction.amount is not None and transaction.transaction_type:
                valid_transactions += 1
            else:
                validation_results['transactions']['issues'].append(f"Transaction {transaction.id} missing required fields")
        
        validation_results['transactions']['valid'] = valid_transactions

        # Calculate overall quality score
        total_records = sum(validation_results[key]['total'] for key in ['leads', 'assignments', 'providers', 'transactions'])
        valid_records = sum(validation_results[key]['valid'] for key in ['leads', 'assignments', 'providers', 'transactions'])
        
        if total_records > 0:
            validation_results['overall_quality'] = valid_records / total_records

        logger.info(f"Data quality validation completed: {validation_results['overall_quality']:.2%} quality")
        
        return {
            'success': True,
            'validation_results': validation_results,
            'timestamp': timezone.now().isoformat()
        }

    except Exception as e:
        logger.error(f"Error validating data quality: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def analyze_user_churn_risk():
    """Analyze churn risk for all providers using ML"""
    from backend.users.models import ProviderProfile
    from backend.leads.ml_services import LeadAccessControlMLService
    from django.utils import timezone
    
    logger.info("Starting churn risk analysis...")
    
    try:
        ml_service = LeadAccessControlMLService()
        
        # Get all active providers
        providers = ProviderProfile.objects.filter(
            verification_status='verified'
        )
        
        high_risk_providers = []
        medium_risk_providers = []
        analyzed_count = 0
        
        for provider in providers:
            try:
                churn_risk = ml_service.predict_user_churn_risk(provider)
                analyzed_count += 1
                
                if churn_risk > 0.7:
                    high_risk_providers.append({
                        'provider_id': provider.id,
                        'email': provider.user.email,
                        'risk_score': churn_risk,
                        'tier': provider.subscription_tier,
                        'leads_used': provider.leads_used_this_month
                    })
                elif churn_risk > 0.4:
                    medium_risk_providers.append({
                        'provider_id': provider.id,
                        'email': provider.user.email,
                        'risk_score': churn_risk,
                        'tier': provider.subscription_tier,
                        'leads_used': provider.leads_used_this_month
                    })
                    
            except Exception as e:
                logger.error(f"Error analyzing churn risk for provider {provider.id}: {str(e)}")
        
        # Log results for review
        logger.info(f"Churn analysis completed: {len(high_risk_providers)} high risk, {len(medium_risk_providers)} medium risk")
        
        return {
            'success': True,
            'analyzed_count': analyzed_count,
            'high_risk_count': len(high_risk_providers),
            'medium_risk_count': len(medium_risk_providers),
            'high_risk_providers': high_risk_providers[:5],  # Top 5 for logging
            'timestamp': timezone.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error in churn risk analysis: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def generate_subscription_recommendations():
    """Generate ML-based subscription upgrade recommendations"""
    from backend.users.models import ProviderProfile
    from backend.leads.ml_services import LeadAccessControlMLService
    from django.utils import timezone
    
    logger.info("Generating subscription recommendations...")
    
    try:
        ml_service = LeadAccessControlMLService()
        
        # Get providers who might benefit from upgrades
        providers = ProviderProfile.objects.filter(
            verification_status='verified',
            is_subscription_active=True,
            subscription_tier__in=['basic', 'advanced', 'pro']  # Exclude enterprise
        )
        
        recommendations_generated = []
        analyzed_count = 0
        
        for provider in providers:
            try:
                recommendations = ml_service.recommend_subscription_upgrade(provider)
                analyzed_count += 1
                
                if recommendations['recommendations']:
                    recommendations_generated.append({
                        'provider_id': provider.id,
                        'email': provider.user.email,
                        'current_tier': recommendations['current_tier'],
                        'recommendations': recommendations['recommendations'],
                        'leads_used': provider.leads_used_this_month
                    })
                    
            except Exception as e:
                logger.error(f"Error generating recommendations for provider {provider.id}: {str(e)}")
        
        logger.info(f"Generated {len(recommendations_generated)} upgrade recommendations")
        
        return {
            'success': True,
            'analyzed_count': analyzed_count,
            'recommendations_count': len(recommendations_generated),
            'sample_recommendations': recommendations_generated[:3],  # Sample for logging
            'timestamp': timezone.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error generating subscription recommendations: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def optimize_lead_distribution():
    """Optimize lead distribution using ML matching"""
    from backend.leads.models import Lead
    from backend.users.models import ProviderProfile
    from backend.leads.ml_services import LeadAccessControlMLService
    from django.utils import timezone
    from datetime import timedelta
    
    logger.info("Optimizing lead distribution...")
    
    try:
        ml_service = LeadAccessControlMLService()
        
        # Get recent leads that need optimization
        recent_leads = Lead.objects.filter(
            created_at__gte=timezone.now() - timedelta(hours=24),
            status='active'
        )
        
        if recent_leads.count() == 0:
            return {
                'success': True,
                'message': 'No recent leads to optimize',
                'timestamp': timezone.now().isoformat()
            }
        
        # Get available providers (filter by subscription_end_date instead of property)
        from django.utils import timezone
        available_providers = ProviderProfile.objects.filter(
            verification_status='verified',
            subscription_end_date__gt=timezone.now()
        )
        
        optimization_results = []
        
        for lead in recent_leads:
            try:
                # Get ML-optimized matches
                matches = ml_service.optimize_lead_matching(lead, available_providers)
                
                if matches:
                    top_match = matches[0]
                    optimization_results.append({
                        'lead_id': lead.id,
                        'lead_title': lead.title,
                        'top_provider': top_match['provider'].user.email,
                        'match_score': top_match['match_score'],
                        'confidence': top_match['confidence'],
                        'reasons': top_match['reasons']
                    })
                    
            except Exception as e:
                logger.error(f"Error optimizing lead {lead.id}: {str(e)}")
        
        logger.info(f"Optimized distribution for {len(optimization_results)} leads")
        
        return {
            'success': True,
            'leads_optimized': len(optimization_results),
            'sample_optimizations': optimization_results[:3],
            'timestamp': timezone.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error optimizing lead distribution: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def adaptive_ml_threshold_adjustment():
    """Adjust ML training thresholds based on data patterns."""
    from backend.leads.models import Lead, LeadAssignment
    from django.conf import settings
    from django.utils import timezone
    from datetime import timedelta

    logger.info("Adjusting ML training thresholds...")

    try:
        # Analyze data patterns over the last 30 days
        recent_leads = Lead.objects.filter(
            created_at__gte=timezone.now() - timedelta(days=30)
        )
        recent_assignments = LeadAssignment.objects.filter(
            assigned_at__gte=timezone.now() - timedelta(days=30)
        )

        # Calculate data growth rates
        leads_count = recent_leads.count()
        assignments_count = recent_assignments.count()

        # Adjust thresholds based on data volume
        current_quality_threshold = getattr(settings, 'ML_MIN_QUALITY_TRAINING_LEADS', 50)
        current_conversion_threshold = getattr(settings, 'ML_MIN_CONVERSION_TRAINING_ASSIGNMENTS', 30)

        # Suggest new thresholds (simplified logic)
        new_quality_threshold = max(20, min(100, leads_count // 2))
        new_conversion_threshold = max(15, min(50, assignments_count // 2))

        adjustment_results = {
            'current_thresholds': {
                'quality_leads': current_quality_threshold,
                'conversion_assignments': current_conversion_threshold
            },
            'suggested_thresholds': {
                'quality_leads': new_quality_threshold,
                'conversion_assignments': new_conversion_threshold
            },
            'data_volume': {
                'recent_leads': leads_count,
                'recent_assignments': assignments_count
            },
            'adjustment_reasoning': {
                'quality_threshold': f"Adjusted based on {leads_count} recent leads",
                'conversion_threshold': f"Adjusted based on {assignments_count} recent assignments"
            }
        }

        logger.info(f"Threshold adjustment completed: Quality {new_quality_threshold}, Conversion {new_conversion_threshold}")
        
        return {
            'success': True,
            'adjustment_results': adjustment_results,
            'timestamp': timezone.now().isoformat()
        }

    except Exception as e:
        logger.error(f"Error adjusting ML thresholds: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def sync_redis_counters_to_database():
    """Sync Redis view counters to database to reduce database lock contention"""
    from django.core.cache import cache
    from django.db import transaction
    from backend.leads.models import Lead
    
    try:
        # Get all Redis keys for lead views
        synced_count = 0
        errors = 0
        
        # Sync the leads that have been recently accessed
        recent_leads = Lead.objects.filter(
            created_at__gte=timezone.now() - timedelta(hours=24)
        ).values_list('id', flat=True)
        
        for lead_id in recent_leads:
            cache_key = f"lead_views_{lead_id}"
            redis_count = cache.get(cache_key)
            
            if redis_count is not None:
                try:
                    with transaction.atomic():
                        lead = Lead.objects.select_for_update(nowait=True).get(id=lead_id)
                        if lead.views_count != redis_count:
                            lead.views_count = redis_count
                            lead.save(update_fields=['views_count'])
                            synced_count += 1
                            logger.info(f"Synced lead {lead_id} views: {redis_count}")
                except Lead.DoesNotExist:
                    # Lead was deleted, remove from cache
                    cache.delete(cache_key)
                except Exception as e:
                    errors += 1
                    logger.warning(f"Failed to sync lead {lead_id}: {str(e)}")
        
        logger.info(f"Redis counter sync completed: {synced_count} synced, {errors} errors")
        return {
            'success': True,
            'synced_count': synced_count,
            'errors': errors,
            'timestamp': timezone.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error syncing Redis counters: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def monitor_and_fix_unassigned_leads():
    """
    PRODUCTION SAFETY: Monitor for unassigned leads and fix them automatically.
    This prevents the 403 unlock errors by ensuring all leads are properly assigned.
    """
    from backend.leads.models import Lead, LeadAssignment
    from backend.leads.services import LeadAssignmentService
    
    try:
        # Find verified leads without assignments (last 6 hours)
        cutoff_time = timezone.now() - timedelta(hours=6)
        
        unassigned_leads = Lead.objects.filter(
            status='verified',
            created_at__gte=cutoff_time
        ).exclude(
            assignments__isnull=False
        ).distinct()
        
        if unassigned_leads.count() == 0:
            logger.info(" No unassigned leads found - system is healthy")
            return {
                'success': True,
                'unassigned_found': 0,
                'fixed': 0,
                'timestamp': timezone.now().isoformat()
            }
        
        logger.warning(f" Found {unassigned_leads.count()} unassigned leads - fixing automatically")
        
        assignment_service = LeadAssignmentService()
        fixed_count = 0
        failed_count = 0
        
        for lead in unassigned_leads:
            try:
                logger.info(f"Auto-fixing unassigned lead: {lead.title} ({lead.id})")
                
                assignments = assignment_service.assign_lead_to_providers(lead.id)
                
                if assignments:
                    fixed_count += 1
                    logger.info(f" Auto-assigned lead {lead.id} to {len(assignments)} providers")
                else:
                    failed_count += 1
                    logger.warning(f" No matching providers for lead {lead.id}")
                    
            except Exception as e:
                failed_count += 1
                logger.error(f" Failed to auto-assign lead {lead.id}: {str(e)}")
        
        logger.info(f" Auto-assignment completed: {fixed_count} fixed, {failed_count} failed")
        
        return {
            'success': True,
            'unassigned_found': unassigned_leads.count(),
            'fixed': fixed_count,
            'failed': failed_count,
            'timestamp': timezone.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error in unassigned lead monitoring: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def bulletproof_flow_check():
    """
    Comprehensive bulletproof flow check - runs every hour to ensure system integrity
    """
    from backend.leads.flow_monitor import flow_monitor
    
    try:
        logger.info(" Starting bulletproof flow check...")
        
        # Run system health check
        health_report = flow_monitor.monitor_system_health()
        
        # Check for critical issues
        critical_alerts = [alert for alert in flow_monitor.get_alerts() if alert['level'] == 'CRITICAL']
        error_alerts = [alert for alert in flow_monitor.get_alerts() if alert['level'] == 'ERROR']
        
        result = {
            'success': True,
            'health_report': health_report,
            'critical_alerts': len(critical_alerts),
            'error_alerts': len(error_alerts),
            'timestamp': timezone.now().isoformat()
        }
        
        # Log critical issues
        if critical_alerts:
            logger.critical(f" {len(critical_alerts)} CRITICAL issues found in bulletproof check!")
            for alert in critical_alerts:
                logger.critical(f" {alert['message']}")
        
        if error_alerts:
            logger.error(f" {len(error_alerts)} ERROR issues found in bulletproof check!")
        
        logger.info(f" Bulletproof check completed")
        return result
        
    except Exception as e:
        logger.critical(f" BULLETPROOF CHECK FAILED: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


